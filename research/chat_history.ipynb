{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45b5e15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_experimental.chat_message_stores.in_memory import InMemoryChatMessageStore\n",
    "from haystack_experimental.components.retrievers import ChatMessageRetriever\n",
    "from haystack_experimental.components.writers import ChatMessageWriter\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.joiners import ListJoiner\n",
    "from haystack import Pipeline, component\n",
    "from typing import List\n",
    "from haystack.components.builders import ChatPromptBuilder\n",
    "import os\n",
    "import json\n",
    "import mlflow\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed0ff184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/12/02 08:58:31 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/12/02 08:58:31 INFO mlflow.store.db.utils: Updating database tables\n",
      "2025-12-02 08:58:31 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "2025-12-02 08:58:31 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2025-12-02 08:58:31 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "2025-12-02 08:58:31 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
      "2025/12/02 08:58:31 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2025/12/02 08:58:31 INFO mlflow.store.db.utils: Updating database tables\n",
      "2025-12-02 08:58:31 INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
      "2025-12-02 08:58:31 INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n"
     ]
    }
   ],
   "source": [
    "#load model tfidf vectorizer\n",
    "mlflow.set_tracking_uri('sqlite:///mlflow.db')\n",
    "tfidf_vectorizer = mlflow.sklearn.load_model(\"models:/tfidf_vectorizer/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bfb31a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_store = InMemoryChatMessageStore()\n",
    "memory_retriever = ChatMessageRetriever(memory_store)\n",
    "memory_writer = ChatMessageWriter(memory_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18989c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = ChatMessage.from_system(\"You are a helpful assistant that answers questions based on the provided context.\")\n",
    "user_message_template = \"\"\"\n",
    "Answer the question based on the user query, please pay attention to the chat history:\n",
    "chat_history:\n",
    "{% for memory in memories %}\n",
    "    {{memory.text}}\n",
    "{% endfor %}\n",
    "\n",
    "query:{{query}}\n",
    "answer:\n",
    "\"\"\"\n",
    "user_message = ChatMessage.from_user(user_message_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497d8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm model\n",
    "@component\n",
    "class GroqLLM:\n",
    "    def __init__(self, model_name=\"meta-llama/llama-4-maverick-17b-128e-instruct\", api_key=None):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    #list of chat message\n",
    "    @component.output_types(output=List[ChatMessage])\n",
    "    def run(self, prompt: List[ChatMessage]):\n",
    "        user_prompt =\"\".join([msg.text for msg in prompt])\n",
    "        \n",
    "        url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": user_prompt}],\n",
    "            \"temperature\": 0.7,\n",
    "            \"max_tokens\": 300\n",
    "        }\n",
    "\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        try:\n",
    "            data = response.json()\n",
    "        except Exception:\n",
    "            raise ValueError(\"Gagal parse JSON dari Groq API: \", response.text)\n",
    "\n",
    "        # Debug untuk melihat isi JSON asli\n",
    "        if \"choices\" not in data:\n",
    "            raise ValueError(\n",
    "                \"Groq API tidak mengembalikan 'choices'.\\n\"\n",
    "                f\"Status Code: {response.status_code}\\n\"\n",
    "                f\"Response JSON:\\n{json.dumps(data, indent=2)}\"\n",
    "            )\n",
    "\n",
    "        # Jika OK, ambil isi respon\n",
    "        result = data[\"choices\"][0][\"message\"][\"content\"]\n",
    "        return {\"output\": [ChatMessage.from_assistant(result)]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d42ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "class PredictionCategory:\n",
    "    def __init__(self,model_name :str , version : int):\n",
    "        self.model_uri = f\"models:/{model_name}_model/{version}\"\n",
    "        self.model = mlflow.sklearn.load_model(self.model_uri)\n",
    "    \n",
    "    @component.output_types(category=str)\n",
    "    def run(self, input_data):\n",
    "        transform_tfidf = tfidf_vectorizer.transform([input_data])\n",
    "        category = self.model.predict(transform_tfidf)\n",
    "        return {\"category\" : category[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df1636b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x0000021E5398C200>\n",
       "ðŸš… Components\n",
       "  - prompt_builder: ChatPromptBuilder\n",
       "  - generator: GroqLLM\n",
       "  - joiner: ListJoiner\n",
       "  - memory_retriever: ChatMessageRetriever\n",
       "  - memory_writer: ChatMessageWriter\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - prompt_builder.prompt -> generator.prompt (list[ChatMessage])\n",
       "  - generator.output -> joiner.values (List[ChatMessage])\n",
       "  - joiner.values -> memory_writer.messages (List[ChatMessage])\n",
       "  - memory_retriever.messages -> prompt_builder.memories (List[ChatMessage])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_llm = GroqLLM(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(\"prompt_builder\", ChatPromptBuilder(variables=[\"query\",\"memories\"], required_variables=[\"query\",\"memories\"]))\n",
    "pipeline.add_component(\"generator\",groq_llm)\n",
    "pipeline.add_component(\"joiner\",ListJoiner(List[ChatMessage]))\n",
    "pipeline.add_component(\"memory_retriever\", memory_retriever)\n",
    "pipeline.add_component(\"memory_writer\", memory_writer)\n",
    "\n",
    "\n",
    "pipeline.connect(\"prompt_builder.prompt\", \"generator.prompt\")\n",
    "pipeline.connect(\"generator.output\", \"joiner\")\n",
    "pipeline.connect(\"joiner\", \"memory_writer\")\n",
    "pipeline.connect(\"memory_retriever\", \"prompt_builder.memories\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d335334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halo! Ada yang bisa saya bantu?\n",
      "Baik! Ada yang bisa saya bantu?\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    messages = [system_message, user_message]\n",
    "    query = input(\"Please input your question or type 'exit' to quit.\\n\")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    res = pipeline.run(\n",
    "        data={\n",
    "            \"prompt_builder\": {\n",
    "                \"query\": query,\n",
    "                \"template\":messages\n",
    "            },\n",
    "            \"joiner\":{\n",
    "                \"values\": [ChatMessage.from_user(query)]\n",
    "            }\n",
    "        },\n",
    "        include_outputs_from=[\"generator\"]\n",
    "    )\n",
    "    response_text = res['generator']['output'][0]._content[0].text\n",
    "    print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a7fce3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
